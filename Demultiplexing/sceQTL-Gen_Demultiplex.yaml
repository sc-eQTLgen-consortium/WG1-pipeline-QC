---
##############################################################################################################
##### The following arguments need to be changed by the user to indicating file locations on your system #####
##############################################################################################################
inputs:
  bind_path: /path/to/bind/to/mount ### List of paths to bind to Singularity. You can specify multiple directories by adding a "," between them. Eg. ${DIRECTORY1},${DIRECTORY2}
  singularity_image: /path/to/singularity/image.sif ### The complete path to the singularity image that has all the softwares
  repo_dir: /path/to/repo/dir/ ### Path to the GitHub repository directory
  poolsheet_path: /path/to/sample/metadata/file.tsv ### tab separated file that has a header. Each line has a pool name used for the scRNA-seq directories, the BAM file, the barcode files, and the number of individuals in each pool
  samplesheet_path: /path/to/sample/metadata/file.tsv ### tab separated file that has a header. Each line has a pool name used for the scRNA-seq directories and the number of individuals in each pool
  individual_list_dir: /path/to/individual_list/directory/ ### Directory that has a different file for each pool. Each file contains a list of the individuals IDs that are in the pool separated by line (match the genotype individual IDs)
  individual_coupling: /path/to/individual/coupling/file.tsv ### tab separated file that has a header. Each line has the pool identifier and the genotype identifier. Only use if the data is not multiplxed.
  vcf: [] ### the path to the genotype file(s) per ancestry

refs:
  ref_dir: /path/to/provided/ref/dir ### This is the path to the directory containing the imputation references provided for the SNP processing and imputation
  alignment_fai: /path/to/alignment/fai ### This is needed to reorder the VCF file in the same order of the aligned BAM.

outputs: 
  output_dir: /path/to/parent/out/dir ### The path to the parent dir where you would like all outputs/results saved. This path must exist before running.

settings:
  is_multiplexed: True ### Whether or not the dataset is multiplexed (i.e. multiple samples per pool)
  sc_data_type: "single-cell" ### The single-cell data type of the input dataset
  rename_chrs: False ### Whether or not to rename the chromosomes from chr1 to 1 of all input BAMs
  reheader_vcf: False ### Whether or not to reorder the chromosomes based on the alignemnt_fai.
  check_sample_swaps: False ### Whether or not to run verifyBamID

cluster_time: {0: "05:59:00", 1: "23:59:00", 2: "6-23:59:00"} ### Options for dynamic time usage for cluster submissions of jobs

##############################################################################################################
##### The following arguments are common parameters that may need to be changed depending on the dataset #####
##############################################################################################################
demultiplex_preprocessing:
  combine_vcfs_all_memory: 4 ### Amount of gigs that will be used for memory and disk space per thread
  combine_vcfs_all_threads: 1 ### Number of threads to use
  combine_vcfs_all_time: 0 ### The time setting to use

  filter4demultiplexing_memory: 5 ### Amount of gigs that will be used for memory and disk space per thread
  filter4demultiplexing_threads: 1 ### Number of threads to use
  filter4demultiplexing_time: 0 ### The time setting to use

  rename_chrs_memory: 16 ### Amount of gigs that will be used for memory and disk space per thread
  rename_chrs_threads: 1 ### Number of threads to use
  rename_chrs_time: 0 ### The time setting to use

  reheader_vcf_memory: 16 ### Amount of gigs that will be used for memory and disk space per thread
  reheader_vcf_threads: 1 ### Number of threads to use
  reheader_vcf_time: 0 ### The time setting to use

  sort4demultiplexing_memory: 32 ### Amount of gigs that will be used for memory and disk space per thread
  sort4demultiplexing_threads: 1 ### Number of threads to use
  sort4demultiplexing_time: 0 ### The time setting to use

  rename_bam_memory: 32 ### Amount of gigs that will be used for memory and disk space per thread
  rename_bam_threads: 1 ### Number of threads to use
  rename_bam_time: 0 ### The time setting to use

  filter_bam_memory: 8 ### This is the number of gigs that will be used for memory when subsetting the bam file.
  filter_bam_threads: 2 ### This is the number of threads that will be used for memory when subsetting the bam file.
  filter_bam_time: 0 ### The time setting to use

popscle:
  popscle_pileup_memory: 10 ### This is the number of gigs that will be used for memory and disk space per thread for the pileup step.
  popscle_pileup_threads: 10 ### The number threads that will be used for the pileup step
  popscle_pileup_time: 0 ### The time setting to use

  popscle_demuxlet_memory: 8 ### This is the number of gigs that will be used for memory and disk space per thread for the demuxlet step
  popscle_demuxlet_threads: 2 ### The number threads that will be used for the demuxlet step
  popscle_demuxlet_time: 0 ### The time setting to use

souporcell:
  souporcell_memory: 8 ### This is the number of gigs that will be used for memory and disk space per thread for the souporcell preflights step
  souporcell_threads: 4 ### The number threads that will be used for the souporcell preflights step
  souporcell_time: 0 ### The time setting to use

  souporcell_preflights_memory: 4 ### This is the number of gigs that will be used for memory and disk space per thread for the souporcell preflights step
  souporcell_preflights_threads: 1 ### The number threads that will be used for the souporcell preflights step
  souporcell_preflights_time: 0 ### The time setting to use

  souporcell_get_bam_regions_memory: 4 ### This is the number of gigs that will be used for memory and disk space per thread for the souporcell get bam regions step
  souporcell_get_bam_regions_threads: 1 ### The number threads that will be used for the souporcell get bam regions step
  souporcell_get_bam_regions_time: 0 ### The time setting to use

  souporcell_remap_splits: 4 ### number of jobs to use for make_fastqs, remap, and retag. Each job gets the memory / threads / time as specified per rule.

  souporcell_make_fastqs_memory: 4 ### This is the number of gigs that will be used for memory and disk space per thread for the souporcell make fastq's step
  souporcell_make_fastqs_threads: 1 ### The number threads that will be used for the souporcell make fastq's step
  souporcell_make_fastqs_time: 0 ### The time setting to use

  souporcell_remap_memory: 16 ### This is the number of gigs that will be used for memory and disk space per thread for the souporcell remap step
  souporcell_remap_threads: 1 ### The number threads that will be used for the souporcell remap step
  souporcell_remap_time: 0 ### The time setting to use

  souporcell_retag_memory: 4 ### This is the number of gigs that will be used for memory and disk space per thread for the souporcell retag step
  souporcell_retag_threads: 1 ### The number threads that will be used for the souporcell retag step
  souporcell_retag_time: 0 ### The time setting to use

  souporcell_samtools_merge_memory: 4 ### This is the number of gigs that will be used for memory and disk space per thread for the merging of the BAM files
  souporcell_samtools_merge_threads: 1 ### The number threads that will be used for the souporcell retag step
  souporcell_samtools_merge_time: 0 ### The time setting to use

  souporcell_freebayes_memory: 4 ### This is the number of gigs that will be used for memory and disk space per thread for the souporcell freebayes step
  souporcell_freebayes_threads: 1 ### The number threads that will be used for the souporcell freebayes step
  souporcell_freebayes_time: 0 ### The time setting to use
  souporcell_freebayes_splits: 4 ### number of jobs to use for Freebayes. Each job gets the memory / threads / time as specified above.

  souporcell_freebayes_combine_memory: 4 ### This is the number of gigs that will be used for memory and disk space per thread for the souporcell freebayes combine step
  souporcell_freebayes_combine_threads: 1 ### The number threads that will be used for the souporcell freebayes combine step
  souporcell_freebayes_combine_time: 0 ### The time setting to use

  souporcell_common_variants_memory: 4 ### This is the number of gigs that will be used for memory and disk space per thread for the souporcell common variants step
  souporcell_common_variants_threads: 1 ### The number threads that will be used for the souporcell common variants step
  souporcell_common_variants_time: 0 ### The time setting to use

  souporcell_vartrix_memory: 16 ### This is the number of gigs that will be used for memory and disk space per thread for the souporcell vartrix step
  souporcell_vartrix_threads: 1 ### The number threads that will be used for the souporcell vartrix step
  souporcell_vartrix_time: 1 ### The time setting to use

  souporcell_souporcell_memory: 4 ### This is the number of gigs that will be used for memory and disk space per thread for the souporcell step
  souporcell_souporcell_threads: 1 ### The number threads that will be used for the souporcell step
  souporcell_souporcell_time: 1 ### The time setting to use

  souporcell_doublets_memory: 4 ### This is the number of gigs that will be used for memory and disk space per thread for the souporcell doublets step
  souporcell_doublets_threads: 1 ### The number threads that will be used for the souporcell doublets step
  souporcell_doublets_time: 0 ### The time setting to use

  souporcell_consensus_memory: 16 ### This is the number of gigs that will be used for memory and disk space per thread for the souporcell consensus step
  souporcell_consensus_threads: 1 ### The number threads that will be used for the souporcell consensus step
  souporcell_consensus_time: 0 ### The time setting to use

  souporcell_summary_memory: 4 ### This is the number of gigs that will be used for memory and disk space per thread for the souporcell sumary step
  souporcell_summary_threads: 1 ### The number threads that will be used for the souporcell summary step
  souporcell_summary_time: 0 ### The time setting to use

  souporcell_pool_vcf_memory: 4 ### This is the number of gigs that will be used for memory and disk space per thread for the souporcell pool VCF step
  souporcell_pool_vcf_threads: 1 ### The number threads that will be used for the souporcell pool VCF step
  souporcell_pool_vcf_time: 0 ### The time setting to use

  souporcell_correlations_memory: 5 ### Amount of memory (in GB) that will be allocated for each thread
  souporcell_correlations_threads: 2 ### The number of threads to be used to run R script that will correlate the souporcell cluster genotypes wiht the reference genotypes
  souporcell_correlations_time: 0 ### The time setting to use

verifybamid:
  verifybamid_memory: 16 ### Amount of memory (in GB) that will be allocated for each thread
  verifybamid_threads: 1 ### The number of threads to be used to check for sample swaps with verifyBamID
  verifybamid_time: 0 ### The time setting to use

  combine_verifybamid_memory: 4 ### Amount of memory (in GB) that will be allocated for each thread
  combine_verifybamid_threads: 1 ### The number threads that will be used for the sample swaps with verifyBamID
  combine_verifybamid_time: 0 ### The time setting to use

scrublet:
  scrublet_memory: 16 ### The number amount of memory that will be used for scrublet doublet detection (in GB)
  scrublet_threads: 1 ### The number threads that will be used for the scrublet step
  scrublet_time: 0 ### The time setting to use
  scrublet_min_gene_variability_pctl: [80, 85, 90, 95] ### Used for gene filtering prior to PCA. Keep the most highly variable genes in the top min_gene_variability_pctl percentile), as measured by the v-statistic [Klein et al., Cell 2015]

  plot_scrublet_memory: 4 ### The number amount of memory that will be used for plot scrublet doublet detection (in GB)
  plot_scrublet_threads: 1 ### The number threads that will be used for the plot scrublet step
  plot_scrublet_time: 0 ### The time setting to use

scds:
  scds_memory: 32 ### The amount of memeory (GB) per thread for the scds step
  scds_threads: 2 ### The number threads that will be used for the scds step
  scds_time: 0 ### The time setting to use

doubletdetection:
  doubletdetection_memory: 16 ### The amount of memory (GB) used per thread for the DoubletDetection step
  doubletdetection_threads: 2 ### The number threads that will be used for the DoubletDetection step
  doubletdetection_time: 0 ### The time setting to use
  doubletdetection_n_iters: [50] ###  Number of fit operations from which to collect p-values

  plot_doubletdetection_memory: 4 ### The amount of memory (GB) used per thread for the plot DoubletDetection step
  plot_doubletdetection_threads: 1 ### The number threads that will be used for the plot DoubletDetection step
  plot_doubletdetection_time: 0 ### The time setting to use

scdblfinder:
  scfblfinder_memory: 16 ### The amount of memeory (GB) per thread for the scDblFinder step
  scdblfinder_threads: 1 ### The number threads that will be used for the scDblFinder step
  scfblfinder_time: 0 ### The time setting to use

doubletfinder:
  doubletfinder_memory: 64 ### The amount of memeory (GB) per thread for the DoubletFinder step
  doubletfinder_threads: 1 ### The number threads that will be used for the DoubletFinder step
  doubletfinder_time: 0 ### The time setting to use

  plot_doubletfinder_memory: 4 ### The amount of memory (GB) used per thread for the plot DoubletFinder step
  plot_doubletfinder_threads: 1 ### The number threads that will be used for the plot DoubletFinder step
  plot_doubletfinder_time: 0 ### The time setting to use

combine_results:
  combine_results_memory: 4 ### The amount of memory (GB) used per thread for the combine results step
  combine_results_threads: 1 ### The number threads that will be used for the combine results step
  combine_results_time: 0 ### The time setting to use

  combine_pools_memory: 4 ### The amount of memory (GB) used per thread for the combine pools step
  combine_pools_threads: 1 ### The number threads that will be used for the combine pools step
  combine_pools_time: 0 ### The time setting to use

  expected_observed_numbers_memory: 4 ### The amount of memory (GB) used per thread for the expected observed numbers step
  expected_observed_numbers_threads: 1 ### The number threads that will be used for the expected observed numbers step
  expected_observed_numbers_time: 0 ### The time setting to use

  plot_singlet_doublet_memory: 4 ### The amount of memory (GB) used per thread for the combine pools step
  plot_singlet_doublet_threads: 1 ### The number threads that will be used for the combine pools step
  plot_singlet_doublet_time: 0 ### The time setting to use

############################################################################################################################################
##### The following arguments in this section should not need to be changed/edited unless the default options do not work your dataset #####
############################################################################################################################################
refs_extra:
  relative_fasta_path: "hg38/ref_genome_QC/Homo_sapiens.GRCh38.dna.primary_assembly.fa"
  relative_hg38_exons_ucsc_bed_path: "hg38exonsUCSC.bed"
  relative_chr_name_conv_path: "chr_name_conv.txt"

settings_extra:
  ignore_file_checks: False ### Boolean to ignore missing file errors in the Snakefile.
  r_memory_buffer: 1 ### Buffer to prevent R memory overflow, available memory is reduced by this amount in GB
  java_memory_buffer: 2 ### Buffer to prevent Java memory overflow, available memory is reduced by this amount in GB
  demultiplexing_methods: ["popscle", "souporcell"] ### Methods to be run for demultiplexing
  sc_doubletdetection_methods: ["DoubletDetection", "scds", "Scrublet"] ### Methods to be run for single-cell input data
  sn_doubletdetection_methods: ["DoubletFinder", "scDblFinder"] ### Methods to be run for single-nucleus input data
  expected_doublet_scaling_factor: 8e-06 ### Proportion of expected doublets. For example see the Chromium User Guide

  tag_group: "CB" ### Tag representing readgroup or cell barcodes, in the case to partition the BAM file into multiple groups. For 10x genomics, use CB
  tag_UMI: "UB" ### Tag representing UMIs. For 10x genomiucs, use UB
  genotype_field: "GP" ### FORMAT field to extract the genotype, likelihood, or posterior from

demultiplex_preprocessing_extra:
  filter4demultiplexing_maf: 0.05 ### The Minor Allele Frequency to filter on before demultiplexing
  filter4demultiplexing_r2: 0.3 ### The R-squared to filter on before demultiplexing

popscle_extra:
  exclude_flag: 1796 ### SAM/BAM flag to exclude
  sam_verbose: 1000000 ### Verbose message frequency for SAM/BAM/CRAM
  vcf_verbose: 10000 ### Verbose message frequency for VCF/BCF
  skip_umi: False ### Do not generate [prefix].umi.gz file, which stores the regions covered by each barcode/UMI pair
  cap_bq: 40 ### (Default demuxlet: 20 but for pileup: 40) Maximum base quality (higher BQ will be capped)
  min_bq: 13 ### Minimum base quality to consider (lower BQ will be skipped)
  min_mq: 20 ### Minimum mapping quality to consider (lower MQ will be ignored)
  min_td: 0 ### Minimum distance to the tail (lower will be ignored)
  excl_flag: 3844 ### SAM/BAM FLAGs to be excluded
  min_total: 0 ### Minimum number of total reads for a droplet/cell to be considered
  min_uniq: 0 ### Minimum number of unique reads (determined by UMI/SNP pair) for a droplet/cell to be considered
  min_snp: 0 ### Minimum number of SNPs with coverage for a droplet/cell to be considered
  geno_error_offset: 0.1 ### Offset of genotype error rate. [error] = [offset] + [1-offset]*[coeff]*[1-r2]
  geno_error_coeff: 0.0 ### Slope of genotype error rate. [error] = [offset] + [1-offset]*[coeff]*[1-r2]
  r2_info: "R2" ### INFO field name representing R2 value. Used for representing imputation quality
  min_mac: 1 ### Minimum minor allele frequency
  min_callrate: 0.5 ### Minimum call rate
  alpha: null ### Grid of alpha to search for (default is 0.1, 0.2, 0.3, 0.4, 0.5)
  doublet_prior: 0.5 ###  Prior of doublet
  min_umi: 0 ### Minimum number of UMIs for a droplet/cell to be considered

souporcell_extra:
  skip_remap: True ### don't remap with minimap2
  no_umi: False ### set to True if your bam has no UMI tag, will ignore/override --umi_tag
  umi_tag: "UB" ### set if your umi tag is not UB
  min_ref: 10 ### min ref to use locus
  min_alt: 10 ### min alt to use locus
  max_loci: 2048 ### max loci per cell, affects speed
  restarts: 100 ### number of restarts in clustering, when there are > 12 clusters we recommend increasing this to avoid local minima
  ploidy: 2 ### ploidy, must be 1 or 2

verifybamid_extra:
  skip_remap: True ### don't remap with minimap2; uses the remapped BAM from the souporcell pipeline.
  geno_error: 0.001 ### error rate of the external genotype file
  min_af: 0.01 ### minimum allele frequency of the markers to include
  min_call_rate: 0.50 ### minimum call rate of the markers to include
  ind_to_compare: "best" ### Options: site (use only site information in the VCF and do not compare with the actual genotypes), self (Only compare the ID-matching individuals between the VCF and BAM file), best (Find the best matching individuals. This option is substantially longer than the default option)
  chip_free: "mix" ### Options: none (Do not perform sequence-only method to estimate parameters) , mix (Estimate contamination using sequence-only method with Brent's single dimensional optimization), refBias (Estimate the reference bias parameters using sequence-only method with Simplex method), full (Estimate both reference bias parameters and the contamination parameters using sequence-only method)
  with_chip: "mix" ### Options: none (Do not perform sequence+array method to estimate parameters), mix (Estimate contamination using sequence+array method with Brent's single dimensional optimization), refBias (Estimate the refernece bias parameters using sequence+array method with Simplex method), full (Estimate both reference bias parameters and the contamination parameters using sequence+array method)
  ignore_rg: False ### ignore the read grouup level comparison and compare samples only (recommended for an expedited run)
  ignore_overlap_pair: False ### ignore overlapping pair end fragment covering the same base. Disabling this option may decrease the sensitivity of the method when the insert size is short (with slight gain in the computational speed)
  no_eof: False ### do not check the EOF marker of the BAM file (for earlier version of BAM)
  precise: False ### calculate the likelihood in log-scale for high-depth data (recommended when   maxDepth is greater than 20. Can be a little bit slower)
  min_map_q: 10 ### minimum mapping quality of the sequence reads to compare
  max_depth: 20 ###
  min_q: 13 ### minimum base quality to include
  max_q: 40 ### maximum base quality to cap
  grid: 0.05 ### the grid interval to search the optimum before running Brent's algorithm.
  ref_ref: 1 ### Initial Pr(refBase|HOMREFGeno) parameter
  ref_het: 0.5 ### Initial Pr(refBase|HETGeno) parameter
  ref_alt: 0 ### Initial Pr(refBase|HOMALTGeno) parameter

doubletfinder_extra:
  dims: 10 ### Number of PCs to use. Estimate significant PCs if null
  resolution: 0.1 ### Value of the resolution parameter, use a value above (below) 1.0 if you want to obtain a larger (smaller) number of communities
  expected_doublet_scaling_factor: 8e-06 ### The fraction of droublets expected based on the number of nuclei recovered
  pn: 0.25 ### Number of doublets to simulate as a proportion of the pool size

scdblfinder_extra:
  expected_doublet_scaling_factor: 8e-06 ### The fraction of droublets expected based on the number of nuclei recovered
  stdev_doublet_rate: null ### The uncertainty range in the doublet rate, interpreted as a +/- around `dbr`. During thresholding, deviation from the expected doublet rate will be calculated from these boundaries, and will be considered null within these boundaries. If NULL, will be 40\% of `dbr`. Set to `dbr.sd=0` to disable the uncertainty around the doublet rate, or to `dbr.sd=1` to disable any expectation of the number of doublets (thus letting the thresholding be entirely driven by the misclassification of artificial doublets)
  nfeatures: 1000 ### The number of top features to use
  dims: 20 ### The number of dimensions used
  removeUnidentifiable: True ### Whether to remove artificial doublets of a combination that is generally found to be unidentifiable
  include_pcs: 10 ### The number of top components to use (e.g. `includePCs=10`, equivalent to 1:10)
  prop_markers: 0 ### The proportion of features to select based on marker identification
  score: "xgb" ### Score to use for final classification
  processing: "default" ### processing Counts (real and artificial) processing before KNN. Either 'default' (normal \code{scater}-based normalization and PCA), 'rawPCA' (PCA without normalization), 'rawFeatures' (no normalization/dimensional reduction), 'normFeatures' (uses normalized features, without PCA), returning a named matrix with cells as rows and components as columns
  metric: "logloss" ###Error metric to optimize during training
  nrounds: 0.25 ### Maximum rounds of boosting
  max_depth: 4 ### Maximum depths of each tree
  iter: 3 ### A positive integer indicating the number of scoring iterations (ignored if `score` isn't based on classifiers). At each iteration, real cells that would be called as doublets are excluding from the training, and new scores are calculated. Recommended values are 1 or 2
  multi_sample_mode: "split" ### Either 'split' (recommended if there is heterogeneity across samples), 'singleModel', 'singleModelSplitThres', or 'asOne'")

doubletdetection_extra:
  boost_rate: 0.25 ### Proportion of cell population size to produce as synthetic doublets
  n_components: 30 ### Number of principal components used for clustering
  n_top_var_genes: 10000 ### Number of highest variance genes to use; other genes discarded. Will use all genes when zero
  replace: False ### If False, a cell will be selected as a synthetic doublet's parent no more than once
  clustering_algorithm: "louvain" # (Default: phenograph) One of `["louvain", "leiden", "phenograph"]`. `"louvain"` and `leiden` refer to the scanpy implementations.
  n_iters: 50 ### (Default: 10) Number of fit operations from which to collect p-values
  pseudocount: 0.1 ### Pseudocount used in normalize_counts. If `1` is used, and `standard_scaling=False`, the classifier is much more memory efficient; however, this may result in fewer doublets detected
  standard_scaling: True ### (Default: False) Set to True to enable standard scaling of normalized count matrix prior to PCA. Recommended when not using Phenograph. Defaults to False
  p_thresh: 1e-16 ### (Default: 1e-7) Hypergeometric test p-value threshold that determines per iteration doublet calls
  voter_thresh: 0.5 ### (Default: 0.9) Fraction of iterations a cell must be called a doublet

scds_extra:
  bcds_ntop: 500 ### Indicating number of top variance genes to consider
  bcds_srat: 1 ### indicating ratio between orginal number of 'cells' and simulated doublets
  bcds_nmax: "tune" ### maximum number of training rounds; integer or 'tune'
  cxds_ntop: 500 ### Indimessageing number of top variance genes to consider
  cxds_binthresh: 0 ### Minimum counts to consider a gene 'present' in a cell

scrublet_extra:
  sim_doublet_ratio: 2.0 ### Number of doublets to simulate relative to the number of observed transcriptomes
  n_neighbors: null ### Number of neighbors used to construct the KNN graph of observed transcriptomes and simulated doublets. If `None`, this is set to round(0.5 * sqrt(n_cells))")
  expected_doublet_scaling_factor: 8e-06 ### The fraction of droublets expected based on the number of nuclei recovered
  stdev_doublet_rate: 0.02 ### Uncertainty in the expected doublet rate
  synthetic_doublet_umi_subsampling: 1.0 ### Rate for sampling UMIs when creating synthetic doublets. If 1.0, each doublet is created by simply adding the UMIs from two randomly  sampled observed transcriptomes. For values less than 1, the  UMI counts are added and then randomly sampled at the specified rate
  get_doublet_neighbor_parents: False ### If True, return the parent transcriptomes that generated the  doublet neighbors of each observed transcriptome. This information can  be used to infer the cell states that generated a given  doublet state
  min_counts: 3 ### Used for gene filtering prior to PCA. Genes expressed at fewer than min_counts in fewer than min_cells are excluded
  min_cells: 3 ### Used for gene filtering prior to PCA. Genes expressed at fewer than min_counts in fewer than are excluded
  min_gene_variability_pctl: 85 ### Used for gene filtering prior to PCA. Keep the most highly variable genes in the top min_gene_variability_pctl percentile), as measured by the v-statistic [Klein et al., Cell 2015]
  log_transform: False ### If True, log-transform the counts matrix (log10(1+TPM)). `sklearn.decomposition.TruncatedSVD` will be used for dimensionality reduction, unless `mean_center` is True
  mean_center: True ### If True, center the data such that each gene has a mean of 0. `sklearn.decomposition.PCA` will be used for dimensionality reduction
  normalize_variance: True ### If True, normalize the data such that each gene has a variance of 1. `sklearn.decomposition.TruncatedSVD` will be used for dimensionality reduction, unless `mean_center` is True
  n_prin_comps: 30 ### Number of principal components used to embed the transcriptomes priorto k-nearest-neighbor graph construction
  doublet_threshold: null ### Manually set the scrublet doublet threshold location. For running a second time if scrublet incorrectly places the threshold the first time

combine_results_extra:
  souporcell_correlation_limit: 0.7 ### The minimum correlation between a cluster and provided SNP genotypes to consider that cluster assigned to that individual
  pct_agreement: 0.7 ### The proportion of a cluster that match the 'ref' assignment to assign that cluster the individual assignment from the reference. Can be between 0.5 and 1. Default is 0.9.
  method: "AnyDoublet" ### (Default: MajoritySinglet) Combination method. Options are 'MajoritySinglet'. 'AtLeastHalfSinglet', 'AnySinglet' or 'AnyDoublet'. We have found that 'MajoritySinglet' provides the most accurate results in most situations and therefore recommend this method. Leave blank if you just want all the softwares to be merged into a single dataframe.
